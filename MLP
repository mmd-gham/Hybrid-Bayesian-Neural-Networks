import tensorflow as tf
from tf_keras.models import Sequential
from tf_keras.layers import Dense, InputLayer, LayerNormalization
from tf_keras.losses import SparseCategoricalCrossentropy
from tf_keras.callbacks import TensorBoard, EarlyStopping
from tf_keras.optimizers import Adam
from tf_keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from imblearn.over_sampling import KMeansSMOTE
import datetime




from google.colab import drive
drive.mount('/content/drive')
np.random.seed(0)
df_analysis = pd.read_csv('/content/drive/My Drive/EpICC/sample_num.csv')
df_rf_cancer = pd.read_csv('/content/drive/MyDrive/Bayesian Model/Supplementary Data/df_rf_cancer.csv')




# Split & Augmention
X = df_rf_cancer.iloc[:, :-1]
y = df_rf_cancer.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

kmeans_smote = KMeansSMOTE(sampling_strategy='not majority', random_state=42, kmeans_estimator=31)
X_resampled, y_resampled = kmeans_smote.fit_resample(X_train, y_train)

X_train = X_resampled
y_train = y_resampled


# Normalization
scaler = StandardScaler()
scaler.fit(X_train)

X_train = tf.convert_to_tensor(scaler.transform(X_train))
X_val= tf.convert_to_tensor(scaler.transform(X_val))
X_test= tf.convert_to_tensor(scaler.transform(X_test))

# one-hot encode
y_train_oh = tf.keras.utils.to_categorical(y_train)
y_test_oh = tf.keras.utils.to_categorical(y_test)
y_val_oh = tf.keras.utils.to_categorical(y_val)



# Display the shape of the training and testing data
print("Training shape:", X_train.shape)
print("Training labels shape:", y_train.shape)
print("y_train_oh shape:", y_train_oh.shape)

print("Testing shape:", X_test.shape)
print("Testing labels shape:", y_test.shape)
print("y_test_oh shape:", y_test_oh.shape)




MLP = Sequential([
    InputLayer(input_shape=(1000,)),

    Dense(256, activation='swish', bias_regularizer=regularizers.L2(1e-4)),
    Dense(128, activation='swish', bias_regularizer=regularizers.L2(1e-4)),
    Dense(128, activation='swish', bias_regularizer=regularizers.L2(1e-4)),

    LayerNormalization(epsilon=1e-6),


    Dense(31, activation='softmax'),
])


MLP.compile(loss="categorical_crossentropy",
              optimizer= Adam(0.0001),
              metrics=['accuracy'],
              experimental_run_tf_function=False)

MLP.summary()


# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)


#Training
start_time = time.time()
history = MLP.fit(X_train, y_train_oh,
                    epochs=30,
                    batch_size=32,
                    validation_data=(X_val, y_val_oh),
                    callbacks=[tensorboard_callback, early_stopping],
                    verbose=1)

end_time = time.time()
training_time = end_time - start_time
print(f"Training time: {training_time:.2f} seconds")



# Plot the training and validation curves
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()




y_pred = MLP.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test_oh, axis=1)


recall = recall_score(y_true, y_pred_labels, average='weighted')
precision = precision_score(y_true, y_pred_labels, average='weighted')
f1 = f1_score(y_true, y_pred_labels, average='weighted')
accuracy = accuracy_score(y_true, y_pred_labels)

print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")
print(f"Accuracy: {accuracy}")




%load_ext tensorboard
%tensorboard --logdir logs/fit
